{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409879fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Read the CSV file and extract the text\n",
    "text_data = []\n",
    "with open('your_csv_file.csv', 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)  # Skip header row if present\n",
    "    for row in reader:\n",
    "        text_data.append(row[0])  # Assuming the text is in the first column\n",
    "\n",
    "# Preprocess the text\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "preprocessed_text = []\n",
    "for text in text_data:\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    preprocessed_text.append(' '.join(tokens))\n",
    "\n",
    "# Keyword extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_text)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "top_keywords = []\n",
    "\n",
    "for i in range(len(text_data)):\n",
    "    feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
    "    tfidf_scores = zip(feature_index, [tfidf_matrix[i, x] for x in feature_index])\n",
    "    sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "    keywords = [feature_names[index] for index, score in sorted_scores[:5]]  # Extract top 5 keywords\n",
    "    top_keywords.append(keywords)\n",
    "\n",
    "# Topic modeling using LDA\n",
    "num_topics = 5  # Number of desired topics\n",
    "lda_model = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "topic_keywords = []\n",
    "\n",
    "for topic_weights in lda_model.components_:\n",
    "    sorted_indexes = topic_weights.argsort()[::-1]\n",
    "    keywords = [feature_names[i] for i in sorted_indexes[:5]]  # Extract top 5 keywords per topic\n",
    "    topic_keywords.append(keywords)\n",
    "\n",
    "# Print top keywords for each document and associated topics\n",
    "for i, (keywords, topic) in enumerate(zip(top_keywords, lda_matrix.argmax(axis=1))):\n",
    "    print(f\"Document {i+1}: Keywords - {', '.join(keywords)} | Topic: {topic}\")\n",
    "\n",
    "# Print the keywords for each topic\n",
    "for i, keywords in enumerate(topic_keywords):\n",
    "    print(f\"Topic {i+1} Keywords: {', '.join(keywords)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
